{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "class KddData(object):\n",
    "\n",
    "    def __init__(self, batch_size, file_name1, file_name2):\n",
    "        data1 = pd.read_csv(file_name1)\n",
    "        data2 = pd.read_csv(file_name2)\n",
    "        \n",
    "        \n",
    "        self._encoder = {\n",
    "            'label':    LabelEncoder()\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        target = np.array(data1['label'])\n",
    "        features = np.array(data1.drop('label', axis=1))\n",
    "        data_X, data_y = self.__encode_data(features, target)\n",
    "        self.train_dataset = TensorDataset(\n",
    "            torch.from_numpy(data_X.astype(np.float32)),\n",
    "            torch.from_numpy(data_y.astype(np.int64))\n",
    "        )\n",
    "\n",
    "        target = np.array(data2['label'])\n",
    "        features = np.array(data2.drop('label', axis=1))\n",
    "        data_X, data_y = self.__encode_data(features, target)\n",
    "        self.test_dataset = TensorDataset(\n",
    "            torch.from_numpy(data_X.astype(np.float32)),\n",
    "            torch.from_numpy(data_y.astype(np.int64))\n",
    "        )\n",
    "\n",
    " \n",
    "        self.train_dataloader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, self.batch_size, shuffle=True)\n",
    "\n",
    "    \"\"\"将数据中字符串部分转换为数字，并将输入的41维特征转换为8*8的矩阵\"\"\"\n",
    "    def __encode_data(self, data_X, data_y):\n",
    "        self._encoder['label'].fit(list(set(data_y)))\n",
    "        data_X = np.pad(data_X, ((0, 0), (0, 100 - len(data_X[0]))), 'constant').reshape(-1, 1, 100)\n",
    "        data_y = self._encoder['label'].transform(data_y)\n",
    "        return data_X, data_y\n",
    "\n",
    "    \"\"\"将数据拆分为训练集和测试集，并转换为TensorDataset对象\"\"\"\n",
    "    def __split_data_to_tensor(self, data_X, data_y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.3)\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.from_numpy(X_train.astype(np.float32)),\n",
    "            torch.from_numpy(y_train.astype(np.int64))\n",
    "        )\n",
    "        test_dataset = TensorDataset(\n",
    "            torch.from_numpy(X_test.astype(np.float32)),\n",
    "            torch.from_numpy(y_test.astype(np.int64))\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    \"\"\"接受一个数组进行解码\"\"\"\n",
    "    def decode(self, data, label=False):\n",
    "        if not label:\n",
    "            _data = list(data)\n",
    "            _data[1] = self._encoder['protocal'].inverse_transform([_data[1]])[0]\n",
    "            _data[2] = self._encoder['service'].inverse_transform([_data[2]])[0]\n",
    "            _data[2] = self._encoder['flag'].inverse_transform([_data[3]])[0]\n",
    "            return _data\n",
    "        return self._encoder['label'].inverse_transform(data)\n",
    "    \n",
    "    def encode(self, data, label=False):\n",
    "        if not label:\n",
    "            _data = list(data)\n",
    "            _data[1] = self._encoder['protocal'].transform([_data[1]])[0]\n",
    "            _data[2] = self._encoder['service'].transform([_data[2]])[0]\n",
    "            _data[3] = self._encoder['flag'].transform([_data[3]])[0]\n",
    "            return _data\n",
    "        return self._encoder['label'].transform([data])[0]\n",
    "\n",
    "batch_size = 64\n",
    "# data_file = out_file\n",
    "# dataset = KddData(batch_size, 'train-expon-nagle-open.csv', 'test-remote-nagle-open-cn2us.csv')\n",
    "# dataset.train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Website Fingerprinting through Deep Learning:         CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(       \n",
    "            nn.Conv1d(in_dim, 32, 8, 1, 0),\n",
    "            nn.BatchNorm1d(32), \n",
    "            nn.ELU(),                     \n",
    "            nn.MaxPool1d(8, 4, 0),\n",
    "            nn.Dropout(0.2), \n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(704, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_class)\n",
    "        )   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "testFile1 = 'test-kr.csv'\n",
    "testFile2 = 'test-us.csv'\n",
    "testFile3 = 'test-wifi.csv'\n",
    "testFile4 = 'test-4G.csv'\n",
    "testFile5 = 'test-3G.csv'\n",
    "\n",
    "\n",
    "dataset = KddData(batch_size, 'train-wired.csv', 'test-kr.csv')\n",
    "model = CNN(1, 2)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "num_epoches = 50\n",
    "USE_GPU = False\n",
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "    global model\n",
    "\n",
    "    if USE_GPU:\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        print('epoch {}'.format(epoch + 1))\n",
    "        print('*' * 10)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(dataset.train_dataloader, 1):\n",
    "            img, label = data\n",
    "            if USE_GPU:\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "            img = Variable(img)\n",
    "            label = Variable(label)\n",
    "            # 向前传播\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            running_loss += loss.item() * label.size(0)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            accuracy = (pred == label).float().mean()\n",
    "            running_acc += num_correct.item()\n",
    "            # 向后传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "            epoch + 1, running_loss / (len(dataset.train_dataset)), running_acc / (len(\n",
    "                dataset.train_dataset))))\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        for data in dataset.test_dataloader:\n",
    "            img, label = data\n",
    "            if USE_GPU:\n",
    "                img = Variable(img, volatile=True).cuda()\n",
    "                label = Variable(label, volatile=True).cuda()\n",
    "            else:\n",
    "                img = Variable(img, volatile=True)\n",
    "                label = Variable(label, volatile=True)\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            eval_loss += loss.item() * label.size(0)\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = (pred == label).sum()\n",
    "            eval_acc += num_correct.item()\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "            dataset.test_dataset)), eval_acc / (len(dataset.test_dataset))))\n",
    "        print()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_736169/2296278748.py:19: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  img = Variable(img, volatile=True)\n",
      "/tmp/ipykernel_736169/2296278748.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  label = Variable(label, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.209139, Acc: 0.952430\n",
      " Recall: 0.9453288316726685, precision: 0.9591598510742188, f1score: 0.9506532549858093\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def runmodel(dataset):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    accuracy = torchmetrics.Accuracy()\n",
    "    recall = torchmetrics.Recall(average='macro', num_classes=2)\n",
    "    precision = torchmetrics.Precision(average='macro', num_classes=2)\n",
    "    f1score = torchmetrics.F1(average='macro', num_classes=2)\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for data in dataset.test_dataloader:\n",
    "        img, label = data\n",
    "        \n",
    "        if USE_GPU:\n",
    "            img = Variable(img, volatile=True).cuda()\n",
    "            label = Variable(label, volatile=True).cuda()\n",
    "        else:\n",
    "            img = Variable(img, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "        # print(label.sum().item(), label.size(0))\n",
    "        # print(label)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.item() * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        eval_acc += num_correct.item()\n",
    "        \n",
    "        label = label.cpu()\n",
    "        pred = pred.cpu()\n",
    "        accuracy(pred, label)\n",
    "        recall(pred, label)\n",
    "        precision(pred, label)\n",
    "        f1score(pred, label)\n",
    "\n",
    "        # label = label.cpu().detach().numpy()\n",
    "        # pred = pred.cpu().detach().numpy()\n",
    "        # for i in range(len(label)):\n",
    "        #     if label[i] == 1:\n",
    "        #         mal_count += 1\n",
    "        #         if pred[i] == 1:\n",
    "        #             mal_correct +=1\n",
    "        #     else:\n",
    "        #         benign_count += 1\n",
    "        #         if pred[i] == 1:\n",
    "        #             benign_error += 1\n",
    "        \n",
    "    print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        dataset.test_dataset)), eval_acc / (len(dataset.test_dataset))))\n",
    "    # print('malicious recall:{:.6f}, false Positive:{:.6f}'.format(mal_correct/mal_count, benign_error/()))\n",
    "    rec = recall.compute().data.detach()\n",
    "    prec = precision.compute().data.detach()\n",
    "    f1sc = f1score.compute().data.detach()\n",
    "    print(f\" Recall: {rec}, precision: {prec}, f1score: {f1sc}\")\n",
    "\n",
    "testFile = 'test-wired.csv'\n",
    "testFile1 = 'test-kr.csv'\n",
    "testFile2 = 'test-us.csv'\n",
    "testFile3 = 'test-wifi.csv'\n",
    "testFile4 = 'test-4G.csv'\n",
    "testFile5 = 'test-3G.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = KddData(batch_size, testFile1, testFile)\n",
    "runmodel(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
